# Sample-efficient-model-based-reinforcement-learning-using-maximum-likelihood
Guided policy search algorithms have proved to work with incredible accuracy for not only controlling a complicated dynamical system, but also learning optimal policies for different unseen instances. In most of algorithms which have been proposed for policy search and learning, one assumes the true nature of the states of the problem. This paper deals with a stastical trajectory optimization procedure for unknown systems and extends it towards learning policies (optimal) which has less noise in them because of the lower variance in the optimal trajectories. Additionally, similarity of maximum likelihood based trajectory optimization methods have been established with that of information theoretic approach of {\it minimax} optimal control. Theoretical evidence regarding optimal policies having less noise in them in comparison to the baselines is also presented and this results in better quality of learning based on widely used performance metrics.
